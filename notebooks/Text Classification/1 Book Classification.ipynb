{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import regex\n",
    "\n",
    "import eventlet\n",
    "from eventlet.green.urllib import request\n",
    "\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wybór i przygotowanie danych\n",
    "\n",
    "Do zaprezentowania klasyfikacji tekstów niezbędnym elementem są oczywiście same teksty. Ponieważ z oczywistych względów nie mogłem oprzeć przykładów na tekstach które są  poufne (a do tej kategorii zaliczają się np. zgłoszenia w systemach CRM) do wykorzystania zostały tylko publicznie dostępne treści. Ponieważ zależało mi na tym aby teksty były polskie oraz łatwo dostępne zdecydowałem się na publikacje w serwisie [wolneletury.pl]. Zadanie będzie polegało na ustaleniu nazwiska autora na podstawie jednego zdania. \n",
    "\n",
    "W pierwszym kroku należy pobrać treści książek oraz podzielić je na pojedyncze zdania i oczywiście do każdego zdania przypisać autora. Wybór padł na następujące publikacje oraz autorów:\n",
    "[wolneletury.pl]: http://wolnelektury.pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_files={\n",
    "    \"Mickiewicz\": [\n",
    "        \"https://wolnelektury.pl/media/book/txt/pan-tadeusz.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/dziady-dziady-widowisko-czesc-i.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/dziady-dziadow-czesci-iii-ustep-do-przyjaciol-moskali.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/ballady-i-romanse-pani-twardowska.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/ballady-i-romanse-powrot-taty.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/ballady-i-romanse-switez.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/dziady-dziady-poema-dziady-czesc-iv.txt\",\n",
    "    ],\n",
    "    \"Sienkiewicz\": [\n",
    "        \"https://wolnelektury.pl/media/book/txt/quo-vadis.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/sienkiewicz-we-mgle.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/potop-tom-pierwszy.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/potop-tom-drugi.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/potop-tom-trzeci.txt\",\n",
    "    ],\n",
    "    \"Orzeszkowa\": [\n",
    "        \"https://wolnelektury.pl/media/book/txt/orzeszkowa-kto-winien.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/nad-niemnem-tom-pierwszy.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/nad-niemnem-tom-drugi.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/nad-niemnem-tom-trzeci.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/gloria-victis-dziwna-historia.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/z-pozogi.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/pani-dudkowa.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/dymy.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/syn-stolarza.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/dobra-pani.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/cnotliwi.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/kilka-slow-o-kobietach.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/patryotyzm-i-kosmopolityzm.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/julianka.txt\",\n",
    "    ],\n",
    "    \"Prus\": [\n",
    "        \"https://wolnelektury.pl/media/book/txt/lalka-tom-drugi.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/lalka-tom-pierwszy.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/antek.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/katarynka.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/prus-anielka.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/prus-placowka.txt\",\n",
    "        \n",
    "    ],\n",
    "    \"Reymont\": [\n",
    "        \"https://wolnelektury.pl/media/book/txt/ziemia-obiecana-tom-pierwszy.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/chlopi-czesc-pierwsza-jesien.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/reymont-chlopi-zima.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/chlopi-czesc-trzecia-wiosna.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/chlopi-czesc-czwarta-lato.txt\",\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wybrane pliki pobierzemy do katalogu data. Do pobrania wykorzystałem bibliotekę [eventlet](http://eventlet.net/doc/) która pozwala na zrównoleglenie intensywnych operacji IO (w tym wypadku pobierania danych z Internetu) z wykorzystaniem tzw. [zielonych wątków](https://pl.wikipedia.org/wiki/Green_thread) (greent hreads). Jest to technika którą w pythonie implementuje się z wykorzystaniem tzw. [współprogramów](https://pl.wikipedia.org/wiki/Wsp%C3%B3%C5%82program) (coroutines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "def fetch(url):\n",
    "    file_path = os.path.join(\"data\",os.path.basename(url))\n",
    "    if os.path.exists(file_path):\n",
    "        return None, None\n",
    "    data = request.urlopen(url).read()\n",
    "    return file_path, data\n",
    "\n",
    "for author in book_files:\n",
    "    pool = eventlet.GreenPool()\n",
    "    \n",
    "    for file_path, data in pool.imap(fetch, book_files[author]):\n",
    "        if file_path:\n",
    "            with open(file_path, mode=\"wb\") as f:\n",
    "                f.write(data)\n",
    "print (\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wstępna obróbka i analiza\n",
    "Tak pobrane pliki z książkami musimy podzielić na zdania, które będziemy wykorzystywać do budowy klasyfikatora. Przy okazji dokonamy ich wstępnej obróbki: zamienimy litery na małe, usuniemy ewentualne znaki specjalne, nadmiarowe spacje itp. Jest to często spotykany jednak dość arbitralny sposób postępowania który w sposób nieodwracalny usuwa z dokumentów (w założeniu mało istotne) informacje. Jako ćwiczenie pozostawię zbadanie wpływu sposobu obróbki na ostateczne rezultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mickiewicz</td>\n",
       "      <td>adam mickiewicz pan tadeusz czyli ostatni zajazd na litwie isbn - - - - księga pierwsza gospodarstwo powrót panicza spotkanie się pierwsze w pokoiku drugie u stołu ważna sędziego nauka o grzeczności podkomorzego uwagi polityczne nad modami początek sporu o kusego i sokoła żale wojskiego ostatni woźny trybunału rzut oka na ówczesny stan polityczny litwy i europy litwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mickiewicz</td>\n",
       "      <td>ojczyzno moja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mickiewicz</td>\n",
       "      <td>ty jesteś jak zdrowie ile cię trzeba cenić ten tylko się dowie kto cię stracił</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mickiewicz</td>\n",
       "      <td>dziś piękność twą w całej ozdobie widzę i opisuję bo tęsknię po tobie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mickiewicz</td>\n",
       "      <td>panno święta co jasnej bronisz częstochowy i w ostrej świecisz bramie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author  \\\n",
       "0  Mickiewicz   \n",
       "1  Mickiewicz   \n",
       "2  Mickiewicz   \n",
       "3  Mickiewicz   \n",
       "4  Mickiewicz   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                 txt  \n",
       "0  adam mickiewicz pan tadeusz czyli ostatni zajazd na litwie isbn - - - - księga pierwsza gospodarstwo powrót panicza spotkanie się pierwsze w pokoiku drugie u stołu ważna sędziego nauka o grzeczności podkomorzego uwagi polityczne nad modami początek sporu o kusego i sokoła żale wojskiego ostatni woźny trybunału rzut oka na ówczesny stan polityczny litwy i europy litwo  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                      ojczyzno moja  \n",
       "2                                                                                                                                                                                                                                                                                                     ty jesteś jak zdrowie ile cię trzeba cenić ten tylko się dowie kto cię stracił  \n",
       "3                                                                                                                                                                                                                                                                                                              dziś piękność twą w całej ozdobie widzę i opisuję bo tęsknię po tobie  \n",
       "4                                                                                                                                                                                                                                                                                                              panno święta co jasnej bronisz częstochowy i w ostrej świecisz bramie  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output corspus file with one sentence per line\n",
    "def preprocess_file(file_path=None, file_url=None):\n",
    "    if not file_path and file_url:\n",
    "        file_path = os.path.join(\"data\",os.path.basename(file_url))\n",
    "        \n",
    "    text = open(file_path,'rb').read().decode(\"utf-8\").lower()\n",
    "\n",
    "    text = regex.sub(u\"[^ \\n\\p{Latin}\\-'.?!]\", \" \",text)\n",
    "    text = regex.sub(u\"[ \\n]+\", \" \", text) # Squeeze spaces and newlines\n",
    "    text = regex.sub(r\"----- ta lektura.*\",\"\", text) # remove footer\n",
    "\n",
    "    return [regex.sub(r\"^ \",\"\",l) for l in regex.split('\\.|,|\\?|!|:',text)]\n",
    "\n",
    "\n",
    "def get_book_df(document, author):\n",
    "    return pd.DataFrame({\n",
    "        'author': pd.Series(len(document)*[author]),\n",
    "        'txt': pd.Series(document),\n",
    "    })\n",
    "    \n",
    "book_lines_df = pd.concat([\n",
    "    get_book_df(preprocess_file(file_url=url),author=author) \n",
    "        for author in book_files for url in book_files[author] \n",
    "])\n",
    "\n",
    "book_lines_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy ile mamy linii danych dla poszczególnych autorów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mickiewicz</th>\n",
       "      <td>5093.0</td>\n",
       "      <td>16.103868</td>\n",
       "      <td>13.951437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orzeszkowa</th>\n",
       "      <td>22177.0</td>\n",
       "      <td>19.400821</td>\n",
       "      <td>17.257917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>219.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prus</th>\n",
       "      <td>31033.0</td>\n",
       "      <td>12.131570</td>\n",
       "      <td>10.041673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reymont</th>\n",
       "      <td>24107.0</td>\n",
       "      <td>16.359398</td>\n",
       "      <td>18.880965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>316.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sienkiewicz</th>\n",
       "      <td>40390.0</td>\n",
       "      <td>13.702377</td>\n",
       "      <td>12.037282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               words                                                   \n",
       "               count       mean        std  min  25%   50%   75%    max\n",
       "author                                                                 \n",
       "Mickiewicz    5093.0  16.103868  13.951437  0.0  6.0  13.0  23.0  145.0\n",
       "Orzeszkowa   22177.0  19.400821  17.257917  0.0  6.0  15.0  27.0  219.0\n",
       "Prus         31033.0  12.131570  10.041673  0.0  5.0  10.0  17.0  133.0\n",
       "Reymont      24107.0  16.359398  18.880965  0.0  5.0  10.0  21.0  316.0\n",
       "Sienkiewicz  40390.0  13.702377  12.037282  0.0  5.0  10.0  19.0  146.0"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_lines_df.groupby('author').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać liczba linii dość mocno się różni. Jest to ważna informacja gdyż [niezrównoważone klasy mają znaczny wpływ na rezultaty osiągane przez znaczną część klasyfikatorów](https://www.svds.com/tbt-learning-imbalanced-classes/). Zanotujmy tą informację aby ją później wykorzystać. Obejrzyjmy też statystyki dotyczące ilości wyrazów w zdaniu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mickiewicz</th>\n",
       "      <td>5093.0</td>\n",
       "      <td>16.103868</td>\n",
       "      <td>13.951437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orzeszkowa</th>\n",
       "      <td>22177.0</td>\n",
       "      <td>19.400821</td>\n",
       "      <td>17.257917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>219.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prus</th>\n",
       "      <td>31033.0</td>\n",
       "      <td>12.131570</td>\n",
       "      <td>10.041673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reymont</th>\n",
       "      <td>24107.0</td>\n",
       "      <td>16.359398</td>\n",
       "      <td>18.880965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>316.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sienkiewicz</th>\n",
       "      <td>40390.0</td>\n",
       "      <td>13.702377</td>\n",
       "      <td>12.037282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count       mean        std  min  25%   50%   75%    max\n",
       "author                                                                 \n",
       "Mickiewicz    5093.0  16.103868  13.951437  0.0  6.0  13.0  23.0  145.0\n",
       "Orzeszkowa   22177.0  19.400821  17.257917  0.0  6.0  15.0  27.0  219.0\n",
       "Prus         31033.0  12.131570  10.041673  0.0  5.0  10.0  17.0  133.0\n",
       "Reymont      24107.0  16.359398  18.880965  0.0  5.0  10.0  21.0  316.0\n",
       "Sienkiewicz  40390.0  13.702377  12.037282  0.0  5.0  10.0  19.0  146.0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_lines_df['words'] = book_lines_df['txt'].apply(\n",
    "    lambda x: len(x.split())\n",
    ")\n",
    "book_lines_df.groupby('author')['words'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierwsza obserwacja jest taka, że  niektóre linie mają zerową długość. Trzeba je więc usunąć. Kolejną jest ciekawostka że Prus miał bardzo spójny styl, najniższą średnią długość zdania (12.13 wyrazów) i najniższe odchylenie standardowe (nie był tez zwolennikiem długich zdań - najdłuższe miało  \"zaledwie\" 133 wyrazy podczas gry u Reymonta było prawie 2 i pół raza dłuższe. Podobnie wygląda też kwestia na 98 percentylu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author\n",
       "Mickiewicz     54.0\n",
       "Orzeszkowa     67.0\n",
       "Prus           39.0\n",
       "Reymont        72.0\n",
       "Sienkiewicz    47.0\n",
       "Name: words, dtype: float64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_lines_df.groupby('author')['words'].quantile(0.98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wystarczy ciekawostek, posprzątajmy dane i weźmy się za przygotowanie modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mickiewicz</th>\n",
       "      <td>5079.0</td>\n",
       "      <td>16.148258</td>\n",
       "      <td>13.944973</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orzeszkowa</th>\n",
       "      <td>22087.0</td>\n",
       "      <td>19.479875</td>\n",
       "      <td>17.248458</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>219.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prus</th>\n",
       "      <td>30941.0</td>\n",
       "      <td>12.167642</td>\n",
       "      <td>10.034745</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reymont</th>\n",
       "      <td>24079.0</td>\n",
       "      <td>16.378421</td>\n",
       "      <td>18.883692</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>316.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sienkiewicz</th>\n",
       "      <td>40262.0</td>\n",
       "      <td>13.745939</td>\n",
       "      <td>12.031542</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count       mean        std  min  25%   50%   75%    max\n",
       "author                                                                 \n",
       "Mickiewicz    5079.0  16.148258  13.944973  1.0  6.0  13.0  23.0  145.0\n",
       "Orzeszkowa   22087.0  19.479875  17.248458  1.0  7.0  15.0  27.0  219.0\n",
       "Prus         30941.0  12.167642  10.034745  1.0  5.0  10.0  17.0  133.0\n",
       "Reymont      24079.0  16.378421  18.883692  1.0  5.0  10.0  21.0  316.0\n",
       "Sienkiewicz  40262.0  13.745939  12.031542  1.0  5.0  10.0  19.0  146.0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_lines_df=book_lines_df[~(book_lines_df['words']==0)]\n",
    "book_lines_df.groupby('author')['words'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ekstrakcja cech i budowa modelu\n",
    "\n",
    "Jednym z najprostszych sposobów na przeprowadzenie klasyfikacji tekstów jest wykorzystanie [regresji logistycznej](https://pl.wikipedia.org/wiki/Regresja_logistyczna). Zanim jednak zabierzemy się do budowy naszego modelu nie zapomnijmy o podzieleniu naszych danych na testowe i treningowe abyśmy mogli zweryfikować wyniki na danych out of sample. Do trenowania wykorzystamy 90% danych, pozostawimy do przeprowadzenia testu na zakończeniu. Przy okazji ważna uwaga, ponieważ chcemy zachować w zbiorach wynikowych proporcje pomiędzy klasami takie jak w danych źródłowych wykorzystamy stratyfikację, czyli najpierw podzielimy zbiór źródłowy na oddzielne zbiory dla każdego autora, na każdym z nich oddzielimy po 10% na zbiór testowy a następnie połączymy odpowiednie części z powrotem. Oczywiście na zakończenie oba zbiory (które formalnie w naszej implementacji są przechowywane jako lista, mają więc ustaloną kolejność) potasujemy. Na szczęście wszystkie te operacje wykona za nas funkcja: [`sklearn.model_selection.train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    book_lines_df, \n",
    "    test_size=0.1,\n",
    "    stratify=book_lines_df['author'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skoro mamy już podzielone dane to pozostaje pytanie, jak wprowadzimy nasz tekst do modelu matematycznego? W Machine Learningu zagadnienie to nosi nazwę [Feature Extraction](https://en.wikipedia.org/wiki/Feature_extraction) i [Feature Engineeringu](https://en.wikipedia.org/wiki/Feature_engineering) (istnieje między nimi szereg subtelnych różnic ale odsyłam do definicji gdyż nie będę ich tu teraz objaśniał). Jest to bardzo obszerny temat, powstało nań wiele książek i publikacji, w naszym prostym przykładzie użyjemy jednego z najprostszych możliwych (a jednocześnie niekoniecznie najgorszych) sposobów zamiany testu na liczby. W pierwszym kroku zbudujemy słownik składający się ze wszystkich słów występujących w tekście. Następnie w każdej próbce (zdaniu) przypiszemy wektor długości takiej jak liczba unikalnych słów i na każdej pozycji odpowiadającej określonemu słowu umieścimy liczbę odpowiadającą ilości wystąpień danego słowa w próbce. Proste, prawda? Taki wektor, jak łatwo sobie wyobrazić, w większości składa się z zer. Całą tę operację wykonuje za nas jedna funkcja:\n",
    "[`sklearn.feature_extraction.text.CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting vector for sentence: 'po dziwie'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x131588 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_df['txt'])\n",
    "sample_sentence = train_df.iloc[2]['txt']\n",
    "print (\"Extracting vector for sentence: '{}'\".format(sample_sentence))\n",
    "vectorizer.transform([sample_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać przykładowe zdanie, składające się ze 2 wyrazów zostało zapisane jako wektor o długości 131588 elementów. Aby przechować taki obiekt w sposób efektywny w pamięci wykorzystano sparse matrix z 2 elementami. Dzięki temu uniknięto konieczności przechowywania 131586 zer :) Przy okazji kolejna ważna informacja, domyślnie tokenizer jako tokeny traktuje wyrazy o minimum 2 znakach więc wszystkie krótsze słowa (podobnie jak znaki specjalne) zostały zignorowane. W słowniku nie znajdziemy więc \"i\", \"na\", \"od\", \"po\" itp. Tego typu słowa mają zazwyczaj niewielką wartość jeśli chodzi o klasyfikację (występują z podobnym prawdopodobieństwem we wszystkich rodzajach tekstów).\n",
    "\n",
    "Jak zapewne zauważyliście do budowy słownika wykorzystałem tylko zbiór treningowy. Dla czego nie cały? Otóż jeśli jakieś słowo występuje wyłącznie w zbiorze testowym nie ma sensu dodawać go do słownika gdyż cecha ta i tak nie stanie się elementem modelu. Ponieważ model nic nie nauczył się na temat tego słowa w danych treningowych nie będzie w stanie tej informacji wykorzystać mimo  iż zostanie ona zakodowana w wektorze wejściowym.\n",
    "\n",
    "Skoro mamy już sposób kodowania zdań na liczby przekształćmy nasze dane, stwórzmy model regresji logistycznej i przeprowadźmy jego dopasowanie (\"trenowanie\" nie byłoby tu właściwym słowem gdyż jest to proces deterministyczny)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=True,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train=vectorizer.transform(train_df['txt'])\n",
    "X_test=vectorizer.transform(test_df['txt'])\n",
    "model = LogisticRegression(class_weight='balanced', dual=True)\n",
    "model.fit(X_train, train_df['author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierwsze dwie operacje zamieniają nasze testy na zbiory wektorów wejściowych. Słowo wyjaśnienia należy się linii trzeciej w której tworzymy model regresji liniowej. Pierwszy z parametrów pomaga zrównoważyć nierównomierne ilości tekstów poszczególnych autorów przypisując im wagi odwrotnie proporcjonalne do częstotliwości występowania danej klasy. Drugi pozwala na wewnętrzne wykorzystanie innego sposobu implementacji algorytmu regresji logistycznej, który jest znacznie szybszy jeśli liczba cech przewyższa ilość próbek (w naszym przypadku mamy 131588 słów w słowniku, czyli kodowanych cech oraz 110520 zdań zbiorze treningowym).\n",
    "\n",
    "Mając gotowy, dopasowany model sprawdźmy jego jakość na danych testowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76123778501628669"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, test_df['author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wynik 76% wydaje się być całkiem niezły biorąc pod uwagę że wykorzystaliśmy jeden z najprostszych sposobów kodowania danych, jeden z najprostszych modeli klasyfikacyjnych a to wszystko na domyślnych ustawieniach. Wyobraźmy sobie że sami stajemy przed takim zadaniem i na podstawie zaledwie kilki słów, do tego w losowej kolejności (model zna tylko ilość wystąpień, nie zna kolejności słów!), musimy określić do którego z 5 autorów należy. Nie wygląda to na proste zadanie. Jako ćwiczenie dla czytelników pozostawię weryfikację wyników dla zdań o określonej minimalnej długości.\n",
    "\n",
    "Trzeba też pamiętać, że accuracy bardzo często [nie jest dobrą miarą](https://www.svds.com/classifiers2/) oceny jakości modelu. Bez wchodzenia w zbyt wiele detali nadmienię że podobnie jest w naszym przypadku. Jako przykład niech posłużą bardziej szczegółowe wyniki miar [precision](https://en.wikipedia.org/wiki/Precision_and_recall), [recall](https://en.wikipedia.org/wiki/Precision_and_recall) (inaczej [sensitivity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity), [czułość](https://pl.wikipedia.org/wiki/Czu%C5%82o%C5%9B%C4%87_i_swoisto%C5%9B%C4%87)), i [f1](https://en.wikipedia.org/wiki/F1_score) dla poszczególnych klas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      " Mickiewicz     0.6286    0.5521    0.5879       509\n",
      " Orzeszkowa     0.7394    0.7344    0.7369      2218\n",
      "       Prus     0.7527    0.7112    0.7314      3103\n",
      "    Reymont     0.7553    0.7578    0.7565      2411\n",
      "Sienkiewicz     0.7955    0.8428    0.8185      4039\n",
      "\n",
      "avg / total     0.7598    0.7612    0.7600     12280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "target = test_df['author']\n",
    "predicted = model.predict(X_test)\n",
    "print (metrics.classification_report(target, predicted, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widać wyraźnie że najgorzej rozpoznawane są zdania Mickiewicza, który był najsłabiej reprezentowany. Tylko w 55% były one prawidłowo rozpoznane (czułość), z 63% precyzją. Najlepiej, oczywiście, model radzi sobie z Sienkiewiczem dla którego mieliśmy najwięcej próbek.\n",
    "\n",
    "W następnej części postaram się opisać kilka sposobów na poprawienie powyższych rezultatów zarówno po stronie cech jak i modelu, pokażę jak możemy połączyć etapy obróbki danych i budowy modelu w jeden proces oraz postaram się napisać odrobinę więcej na temat sposobów ewaluacji modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
